import json
import re
import torch
from transformers import MarianMTModel, MarianTokenizer
import os

# ‚úÖ Load model
print("üîÑ Loading model...")
model_name = "Helsinki-NLP/opus-mt-en-hi"
tokenizer = MarianTokenizer.from_pretrained(model_name)
model = MarianMTModel.from_pretrained(model_name)
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model = model.to(device)
print(f"‚úÖ Model loaded on {device}")

# ‚úÖ Hindi disclaimer
DISCLAIMER_HI = "‡§Ø‡§π ‡§ú‡§æ‡§®‡§ï‡§æ‡§∞‡•Ä ‡§ï‡•á‡§µ‡§≤ ‡§∂‡•à‡§ï‡•ç‡§∑‡§ø‡§ï ‡§â‡§¶‡•ç‡§¶‡•á‡§∂‡•ç‡§Ø‡•ã‡§Ç ‡§ï‡•á ‡§≤‡§ø‡§è ‡§π‡•à ‡§î‡§∞ ‡§á‡§∏‡•á ‡§ö‡§ø‡§ï‡§ø‡§§‡•ç‡§∏‡•Ä‡§Ø ‡§∏‡§≤‡§æ‡§π ‡§®‡§π‡•Ä‡§Ç ‡§Æ‡§æ‡§®‡§æ ‡§ú‡§æ‡§®‡§æ ‡§ö‡§æ‡§π‡§ø‡§è‡•§ ‡§ï‡•É‡§™‡§Ø‡§æ ‡§µ‡•ç‡§Ø‡§ï‡•ç‡§§‡§ø‡§ó‡§§ ‡§Æ‡§æ‡§∞‡•ç‡§ó‡§¶‡§∞‡•ç‡§∂‡§® ‡§ï‡•á ‡§≤‡§ø‡§è ‡§ï‡§ø‡§∏‡•Ä ‡§∏‡•ç‡§µ‡§æ‡§∏‡•ç‡§•‡•ç‡§Ø ‡§µ‡§ø‡§∂‡•á‡§∑‡§ú‡•ç‡§û ‡§∏‡•á ‡§™‡§∞‡§æ‡§Æ‡§∞‡•ç‡§∂ ‡§≤‡•á‡§Ç‡•§"

# ‚úÖ Entity protection
ENTITY_MAP = {
    "LCMV": "__LCMV__",
    "Taenia solium": "__TAENIA__",
    "Whipworm": "__WHIPWORM__"
}

def protect_entities(text):
    for k, v in ENTITY_MAP.items():
        text = text.replace(k, v)
    return text

def restore_entities(text):
    for k, v in ENTITY_MAP.items():
        text = text.replace(v, k)
    return text

def strip_disclaimer(text):
    return re.sub(r"This information.*?guidance\.", "", text, flags=re.I).strip()

def batch_translate(texts):
    inputs = tokenizer(texts, return_tensors="pt", padding=True, truncation=True).to(device)
    outputs = model.generate(**inputs, max_length=256)
    return [tokenizer.decode(t, skip_special_tokens=True) for t in outputs]

# ‚úÖ File paths
weak_file = "intent_hi_debug_failed.json"
patched_file = "intent_hi_debug_patched.json"

# ‚úÖ Load weak entries
print(f"üìÅ Loading weak entries from {weak_file}...")
with open(weak_file, "r", encoding="utf-8") as f:
    weak_data = json.load(f)

# ‚úÖ Load existing patched entries if resuming
if os.path.exists(patched_file):
    print(f"üîÑ Resuming from existing {patched_file}...")
    with open(patched_file, "r", encoding="utf-8") as f:
        patched_dict = json.load(f)
else:
    patched_dict = {}

total = sum(len(v) for v in weak_data.values())
print(f"üîç Found {total} weak entries to patch")

# ‚úÖ Resume logic
for intent_group, entries in weak_data.items():
    patched_entries = patched_dict.get(intent_group, [])
    already_done = set(e["en"] for e in patched_entries)
    batch = []

    for i, entry in enumerate(entries):
        en_text = entry.get("en", "").strip()
        if not en_text or en_text in already_done:
            continue

        informative_en = strip_disclaimer(en_text)
        protected_en = protect_entities(informative_en)
        batch.append((entry, protected_en))

        if len(batch) == 50:
            print(f"üöÄ Translating batch {i+1}/{len(entries)} in {intent_group}...")
            texts = [b[1] for b in batch]
            translations = batch_translate(texts)
            for (entry, _), hi_raw in zip(batch, translations):
                hi_final = f"{restore_entities(hi_raw).strip()} {DISCLAIMER_HI}"
                entry["hi"] = hi_final
                entry["notes"] = "Regenerated with Helsinki-NLP"
                patched_entries.append(entry)
            batch = []

            # ‚úÖ Save progress every 500 entries
            if len(patched_entries) % 500 == 0:
                patched_dict[intent_group] = patched_entries
                with open(patched_file, "w", encoding="utf-8") as f:
                    json.dump(patched_dict, f, indent=2, ensure_ascii=False)
                print(f"üíæ Checkpoint saved at {len(patched_entries)} entries for {intent_group}")

    # Final batch
    if batch:
        print(f"üöÄ Translating final batch for {intent_group}...")
        texts = [b[1] for b in batch]
        translations = batch_translate(texts)
        for (entry, _), hi_raw in zip(batch, translations):
            hi_final = f"{restore_entities(hi_raw).strip()} {DISCLAIMER_HI}"
            entry["hi"] = hi_final
            entry["notes"] = "Regenerated with Helsinki-NLP"
            patched_entries.append(entry)

    patched_dict[intent_group] = patched_entries

# ‚úÖ Final save
print(f"üíæ Saving final patched file to {patched_file}...")
with open(patched_file, "w", encoding="utf-8") as f:
    json.dump(patched_dict, f, indent=2, ensure_ascii=False)

print(f"‚úÖ Done! Patched {sum(len(v) for v in patched_dict.values())} entries.")
